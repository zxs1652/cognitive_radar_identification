{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the autoencoder model\n",
    "# Define the CNN-based autoencoder model\n",
    "class CNN_Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(2, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(8, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16, 2, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m iRadar \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m train_data_input \u001b[38;5;241m=\u001b[39m rho123[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m---> 34\u001b[0m train_data_input \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmax(train_data_input)\n\u001b[0;32m     35\u001b[0m train_data_output \u001b[38;5;241m=\u001b[39m received_power_1[iRadar]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m train_data_output \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(train_data_output)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# Generate sample data (300 pairs of 1x300 input and 1x300 output)\n",
    "# np.random.seed(0)\n",
    "# train_data_input = np.random.rand(300, 1, 300)  # 1x300 input features\n",
    "# train_data_output = np.random.rand(300, 1, 300)  # 1x300 output features\n",
    "\n",
    "# Combine input and output for training the autoencoder\n",
    "# train_data = np.concatenate((train_data_input, train_data_output), axis=1)  # Combined 2x300 data\n",
    "# train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# load data from .mat\n",
    "prx1_data = loadmat('prx1_800_rng3.mat')\n",
    "prx4_data = loadmat('prx1_800_rng5.mat')\n",
    "rho123_data = loadmat('rho_800_rng3.mat')\n",
    "rho456_data = loadmat('rho_800_rng5.mat')\n",
    "tarpos_data = loadmat('tarpos_800_rng3.mat')\n",
    "\n",
    "\n",
    "# reformat the training data\n",
    "i_dataset = 0\n",
    "received_power_1 = prx1_data['prx1'][i_dataset][0]\n",
    "rho123 = rho123_data['rho'][i_dataset][0]\n",
    "transmitted_power_1 = received_power_1 * (rho123 ** 2)\n",
    "tar_pos = tarpos_data['target_pos'][i_dataset][0].T\n",
    "# reformat the test data\n",
    "received_power_4 = prx4_data['prx1'][i_dataset][0]\n",
    "rho123 = rho123_data['rho'][i_dataset][0]\n",
    "# transmitted_power_4 = received_power_1 * (rho ** 2)\n",
    "\n",
    "iRadar = 0\n",
    "train_data_input = rho123[0].reshape(1,-1) ** 4\n",
    "train_data_input /= np.max(train_data_input)\n",
    "train_data_output = received_power_1[iRadar].reshape(1,-1)\n",
    "train_data_output /= np.max(train_data_output)\n",
    "train_data = np.concatenate((train_data_input, train_data_output), axis=0)  # Combined 2x300 data\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "model = CNN_Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the autoencoder\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Generate new data (2x100)\n",
    "new_data_input = np.random.rand(50, 1, 100)\n",
    "new_data_output = np.random.rand(50, 1, 100)\n",
    "new_data = np.concatenate((new_data_input, new_data_output), axis=1)  # Combined 2x100 new data\n",
    "new_data = torch.tensor(new_data, dtype=torch.float32)\n",
    "\n",
    "# Pad new data to match the training data size (2x300)\n",
    "padding = (0, train_data.shape[2] - new_data.shape[2])\n",
    "padded_new_data = torch.nn.functional.pad(new_data, padding, \"constant\", 0)\n",
    "\n",
    "# Calculate reconstruction error for new data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(padded_new_data)\n",
    "    unpadded_reconstructions = reconstructions[:, :, :new_data.shape[2]]  # Unpad the output to original size\n",
    "    reconstruction_error = torch.mean((new_data - unpadded_reconstructions) ** 2, dim=[1, 2])\n",
    "\n",
    "# Determine a threshold for reconstruction error based on training data\n",
    "with torch.no_grad():\n",
    "    train_reconstructions = model(train_data)\n",
    "    train_error = torch.mean((train_data - train_reconstructions) ** 2, dim=[1, 2])\n",
    "    threshold = torch.mean(train_error) + 2 * torch.std(train_error)\n",
    "\n",
    "# Check if new data is within the threshold\n",
    "new_data_within_distribution = reconstruction_error < threshold\n",
    "\n",
    "# Output results\n",
    "print(\"Reconstruction Error for New Data:\", reconstruction_error)\n",
    "print(\"Threshold:\", threshold)\n",
    "print(\"New Data Within Distribution:\", new_data_within_distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
